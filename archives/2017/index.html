<!DOCTYPE HTML>

    <html lang="zh-Hans">
  
<head>
  <meta charset="utf-8">
  
  <title>彙整：2017 | CastellanZhang&#39;s blog</title>
  <meta name="author" content="CastellanZhang">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="CastellanZhang&#39;s blog"/>

  
    <meta property="og:image" content="undefined"/>
  

  
    <meta http-equiv="Content-Language" content="zh-Hans"/>
  

  <link href="/img/favicon.png" rel="icon">
  
    <link rel="apple-touch-icon" href="/img/apple-icon.png">
    <link rel="apple-touch-icon-precomposed" href="/img/apple-icon.png">
    

  <link rel="alternate" href="/atom.xml" title="CastellanZhang&#39;s blog" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  
  <style type="text/css">
  /* Tim Pietrusky advanced checkbox hack (Android <= 4.1.2) */
body{ -webkit-animation: bugfix infinite 1s; }
@-webkit-keyframes bugfix { from {padding:0;} to {padding:0;} }

  
  <!-- Chinese readability improvements -->
    article {font-weight: 400;letter-spacing: .01rem;}
    article .entry{line-height:2;}
  

  
    article .post-content-index .entry{max-height: 550px; overflow:hidden;}
  
</style>

  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'null', 'auto');
  ga('send', 'pageview');
 
</script>




  
    <!-- 360 Font and Baidu CDN in China -->
    
      <link href='http://fonts.useso.com/css?family=Open+Sans:300,400|Playball' rel='stylesheet' type='text/css'>
    
  <link href='http://apps.bdimg.com/libs/fontawesome/4.1.0/css/font-awesome.css' rel='stylesheet' type='text/css'>
  <script src="http://libs.baidu.com/jquery/1.11.1/jquery.min.js"></script>
  



</head>


<body>
  <header id="header" class="inner"><div class="padding">
	<div class="alignleft logo">
	  <h1><a href="/">CastellanZhang&#39;s blog</a></h1>
	</div>
	<nav id="main-nav" class="alignright">
		<input type="checkbox" id="toggle" />
		<label for="toggle" class="toggle" data-open="Main Menu" data-close="Close Menu" onclick><i class="fa fa-bars"></i></label>
	  <ul class="menu">
	    
	      <li><a href="/">Home</a></li>
	    
	      <li><a href="/archives">Archives</a></li>
	    
	    
	  </ul>
	</nav>
	<div class="clearfix"></div>
</div>
</header>
  <div id="page-heading-wrap">
  	<div class="inner">
      <div class="padding">
    		
          <h2></h2>
        
      </div>
  	</div>
  </div>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper" class="padding">
<h2 class="archive-title">2017</h2>


  
    <article class="post">
  
  
    <div class="post-content-index">
  
      
        <header>
          <div class="icon"></div>
            
    
      <h1 class="title transition"><a href="/2017/07/16/lambdafm/">lambdaFM</a></h1>
    
  
          <ul>
            <li>
              <span class="heading-span">Posted on: </span>
              <time datetime="2017-07-16T04:28:39.000Z">Jul 16 2017</time>
            </li>
            
              <li>
                <span class="heading-span">By: </span>

                
                  <a href="/">CastellanZhang</a>
                

              </li>
            
            <li>
              <span class="heading-span">With: </span>
              
          </ul>
        </header>
      
      <div class="entry">
        
          
            <h1 id="lambdaFM"><a href="#lambdaFM" class="headerlink" title="lambdaFM"></a>lambdaFM</h1><h2 id="u524D_u8A00"><a href="#u524D_u8A00" class="headerlink" title="前言"></a>前言</h2><p>　　前阵子github上有人问我能不能实现一下基于FM的排序模型，于是周末就在<a href="https://github.com/CastellanZhang/alphaFM" target="_blank" rel="external">alphaFM</a>基础上修改一番，类似lambdaMART的思路，把lambdaRank和FM结合，实现了lambdaFM。</p>
<p>　　代码地址以及使用方法在：<br>　　<a href="https://github.com/CastellanZhang/lambdaFM" target="_blank" rel="external">https://github.com/CastellanZhang/lambdaFM</a></p>
<p>　　同样是FTRL的online learning优化方法，支持高维稀疏特征，单机多线程版本。</p>
<p>　　lambdaFM同时也实现了pairwise的算法，即不考虑deltaNDCG，可以通过参数-rank来选择使用lambdaRank还是pairwise。</p>
<h2 id="pairwise"><a href="#pairwise" class="headerlink" title="pairwise"></a>pairwise</h2><p>　　让我们从pairwise说起，以搜索为例，对于一对样本 $\langle x^i,x^j\rangle$ ，如果 $x^i$ 的相关性好于 $x^j$，我们记为 $x^i\triangleright x^j$ ,反之为 $x^i\triangleleft x^j$ ，相应目标值 $y_{ij}$ 为1或0。建立概率模型如下：<br>$$<br>P_{ij}=P(x^i\triangleright x^j|\Theta)=P(y_{ij}=1|\langle x^i,x^j\rangle,\Theta)\\<br>=\sigma(\hat{y}(x^i|\Theta)-\hat{y}(x^j|\Theta))=\frac{1}{1+e^{-(\hat{y}(x^i|\Theta)-\hat{y}(x^j|\Theta))}}<br>$$<br>　　其中 $\hat{y}(x|\Theta)$ 就是FM的输出：<br>$$<br>\hat{y}(x|\Theta):=w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^n\sum_{j=i+1}^n\langle v_i,v_j\rangle x_ix_j\\<br>=w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^n\sum_{j=i+1}^nx_ix_j\sum_{f=1}^kv_{i,f}v_{j,f}\\<br>=w_0+\sum_{i=1}^nw_ix_i+\frac{1}{2}\sum_{f=1}^k\left(\left(\sum_{i=1}^nv_{i,f}x_i\right)^2-\sum_{i=1}^nv_{i,f}^2x_i^2\right)<br>$$<br>　　模型参数估计仍然用最大似然，对于所有训练样本对集合 $S$ ，最优化问题为：<br>$$<br>\mathop{\arg\max}_{\Theta}\prod_{(\langle x^i,x^j\rangle,y_{ij})\in S}P_{ij}^{y_{ij}}(1-P_{ij})^{1-y_{ij}}<br>$$<br>　　对于训练样本集合 $S$ ，我们总可以调整每一对样本的顺序使得总是有 $x^i\triangleright x^j$ ，即所有的 $y_{ij}$ 都等于1，上面公式简化为：<br>$$<br>\mathop{\arg\max}_{\Theta}\prod_{(\langle x^i,x^j\rangle,1)\in S}P_{ij}=\mathop{\arg\min}_{\Theta}\sum_{(\langle x^i,x^j\rangle,1)\in S}-\ln P_{ij}<br>$$<br>　　这样每一对样本 $\langle x^i,x^j\rangle$ 的损失函数为：<br>$$<br>l(\Theta|\langle x^i,x^j\rangle)=-\ln P_{ij}=\ln (1+e^{-(\hat{y}(x^i|\Theta)-\hat{y}(x^j|\Theta))})<br>$$<br>　　损失函数对参数求偏导数：<br>$$<br>\frac{\partial l}{\partial\theta}=<br>\frac{\partial l}{\partial(\hat{y}(x^i|\Theta)-\hat{y}(x^j|\Theta))}(\frac{\partial\hat{y}(x^i|\Theta)}{\partial\theta}-\frac{\partial\hat{y}(x^j|\Theta)}{\partial\theta})<br>$$<br>　　我们令：<br>$$<br>\lambda_{ij}=\frac{\partial l}{\partial(\hat{y}(x^i|\Theta)-\hat{y}(x^j|\Theta))}=-\frac{1}{1+e^{\hat{y}(x^i|\Theta)-\hat{y}(x^j|\Theta)}}<br>$$<br>　　则上面公式可以简化为：<br>$$<br>\frac{\partial l}{\partial\theta}=<br>\lambda_{ij}(\frac{\partial\hat{y}(x^i|\Theta)}{\partial\theta}-\frac{\partial\hat{y}(x^j|\Theta)}{\partial\theta})<br>$$<br>　　而FM的输出对参数的偏导数在alphaFM的介绍中已经给出过，下面直接列出。注意，由于是pair的方法，偏置项 $w_0$ 相减总会抵消掉，我们可以固定 $w_0$ 为0，不需要再求解，也就不需要对 $w_0$ 再算偏导数：<br>$$<br>\frac{\partial\hat{y}}{\partial\theta}=<br>\begin{cases}<br>x_i, &amp; if\,\,\theta\,\,is\,\,w_i \\<br>x_i\sum_{j=1}^nv_{j,f}x_j-v_{i,f}x_i^2 &amp; if\,\,\theta\,\,is\,\,v_{i,f} \\<br>\end{cases}<br>$$<br>　　有了损失函数对参数的偏导，后面优化就是水到渠成。</p>
<h2 id="lambdaRank"><a href="#lambdaRank" class="headerlink" title="lambdaRank"></a>lambdaRank</h2><p>　　在上面的pairwise方法中，可以看到对于每一对样本都是同等对待的，算法尽量使得每一对样本的label大小关系都能预测对，而对于它们具体的label是多少以及在展现中的位置并不敏感。但在实际问题中，当评价指标是NDCG等时，这些信息就很重要了。举个例子：比如同一query下召回了4个doc，实际相关性分数分别为0 1 3 4，我们有两个排序模型A和B，通过模型的打分，排列结果分别为4 3 0 1和3 4 1 0。从pairwise的角度来看，两个排列跟最优排列4 3 1 0都只相差一次交换，似乎模型效果没差别，但是从NDCG的角度来看，NDCG(A) = 0.997，NDCG(B) = 0.852，明显模型A的效果更好。</p>
<p>　　为了迎合NDCG指标，需要在训练的时候对样本pair区别对待，设置不同的权重，lambdaRank提出了一种权重deltaNDCG，即在原来的顺序上如果交换样本 $i$ 和样本 $j$ ，带来的NDCG值变化的绝对值。形式上是将上面公式的 $\lambda_{ij}$ 乘以权重系数，变成 $\lambda_{ij}’$ ：<br>$$<br>\lambda_{ij}’=\lambda_{ij}|\Delta NDCG_{ij}|<br>$$<br>　　这应该就是lambda名字的来源。在我看来，lambdaRank其实仍然是一种pairwise的方法，因为并没有直接对list做优化，跟ListNet等listwise方法有本质不同。</p>

          
        
      </div>
      <footer>
        
          <div class="alignright">
            <a href="/2017/07/16/lambdafm/#more" class="more-link">Continue Reading<i class="fa fa-long-arrow-right fa-1"></i></a>
          </div>
        
        <div class="clearfix"></div>
      </footer>
    </div>
</article>



  
    <article class="post">
  
  
    <div class="post-content-index">
  
      
        <header>
          <div class="icon"></div>
            
    
      <h1 class="title transition"><a href="/2017/06/01/mlr_plm/">MLR, PLM</a></h1>
    
  
          <ul>
            <li>
              <span class="heading-span">Posted on: </span>
              <time datetime="2017-06-01T15:25:17.000Z">Jun 1 2017</time>
            </li>
            
              <li>
                <span class="heading-span">By: </span>

                
                  <a href="/">CastellanZhang</a>
                

              </li>
            
            <li>
              <span class="heading-span">With: </span>
              
          </ul>
        </header>
      
      <div class="entry">
        
          
            <h1 id="MLR_2C_PLM"><a href="#MLR_2C_PLM" class="headerlink" title="MLR, PLM"></a>MLR, PLM</h1><h2 id="u524D_u8A00"><a href="#u524D_u8A00" class="headerlink" title="前言"></a>前言</h2><p>　　最近阿里的盖坤大神放出了一篇论文Learning Piece-wise Linear Models from Large Scale Data for Ad Click Prediction，介绍了阿里广告的一个主要ctr预估模型Large Scale Piece-wise Linear Model (LS-PLM)，在2012年就开始使用，据说早期叫做Mixture of LR(MLR)。</p>
<p>　　看完论文就很想验证一下效果，于是基于原来的<a href="https://github.com/CastellanZhang/alphaFM" target="_blank" rel="external">alphaFM</a>代码很快就实现了一个单机多线程版本，优化算法用了FTRL。完成代码的时候正好又赶上alphaGo完虐人类，于是命名仍然冠以alpha，叫做alphaPLM。</p>
<p>　　代码地址在：<br>　　<a href="https://github.com/CastellanZhang/alphaPLM" target="_blank" rel="external">https://github.com/CastellanZhang/alphaPLM</a></p>
<h2 id="u7B97_u6CD5_u539F_u7406"><a href="#u7B97_u6CD5_u539F_u7406" class="headerlink" title="算法原理"></a>算法原理</h2><p>　　PLM可以看做是混合了聚类和分类的思想，即将特征空间分片或者说分区间，每个分片就是一个聚类，每个聚类对应一个单独的线性模型LR。这里的聚类是软聚类，即每个样本可以属于多个分片，有概率分布。最后计算ctr是先算出在每个分片的ctr，再按属于各个分片的概率加权平均。通过分片线性拟合，达到了非线性的效果。</p>
<p>　　具体模型公式如下：</p>
<p>$$<br>p(y=1|x)=\sum_{i=1}^{m}\frac{e^{u_i^Tx}}{\sum_{j=1}^me^{u_j^Tx}}\cdot\frac{1}{1+e^{-w_i^Tx}}\\<br>=\sum_{i=1}^{m}\frac{e^{u_i^Tx}}{\sum_{j=1}^me^{u_j^Tx}}\cdot\sigma(w_i^Tx)<br>$$</p>
<p>　　可以看到，聚类部分是用了softmax函数，分类部分就是LR的sigmoid函数。该算法的巧妙之处就是将二者合成一个公式，一起训练参数。公式中m是分片数，属于超参数，由人工给定。模型参数是 $\Theta=\{u_1,…,u_m,w_1,…,w_m\}\in R^{d\times 2m}$，需要训练得到。</p>
<p>　　论文中的优化方法采用的是LBFGS，实现起来比较复杂，我为了快速验证算法效果，优化改成了FTRL，需要计算损失函数对u和w的梯度，下面给出推导。</p>
<p>　　首先对于 $y\in\{-1,1\}$，模型可以统一形式：</p>
<p>$$<br>p(y|x)=\sum_{i=1}^{m}\frac{e^{u_i^Tx}}{\sum_{j=1}^me^{u_j^Tx}}\cdot\frac{1}{1+e^{-yw_i^Tx}}\\<br>=\sum_{i=1}^{m}\frac{e^{u_i^Tx}}{\sum_{j=1}^me^{u_j^Tx}}\cdot\sigma(yw_i^Tx)<br>$$</p>
<p>　　单条样本(x,y)的损失函数：<br>$$<br>l(\Theta|x,y)=-\ln P(y|x,\Theta)=-\ln\frac{1}{\sum_{j=1}^me^{u_j^Tx}}\sum_{i=1}^{m}e^{u_i^Tx}\sigma(yw_i^Tx)\\<br>=\ln\sum_{j=1}^me^{u_j^Tx}-\ln(\sum_{i=1}^{m}e^{u_i^Tx}\sigma(yw_i^Tx))<br>$$<br>　　梯度计算：<br>$$<br>\nabla_{u_k}l=\frac{e^{u_k^Tx}x}{\sum_{j=1}^me^{u_j^Tx}}-\frac{e^{u_k^Tx}\sigma(yw_k^Tx)x}{\sum_{i=1}^{m}e^{u_i^Tx}\sigma(yw_i^Tx)}<br>$$<br>$$<br>\nabla_{w_k}l=\frac{ye^{u_k^Tx}\sigma(yw_k^Tx)(\sigma(yw_k^Tx)-1)x}{\sum_{i=1}^{m}e^{u_i^Tx}\sigma(yw_i^Tx)}<br>$$<br>　　后续的FTRL算法框架和alphaFM非常类似，不再详述，可以参见之前的<a href="http://castellanzhang.github.io/2016/10/16/fm_ftrl_softmax/" target="_blank" rel="external">文档</a>和实现代码。</p>
<h2 id="u7B97_u6CD5_u6548_u679C"><a href="#u7B97_u6CD5_u6548_u679C" class="headerlink" title="算法效果"></a>算法效果</h2><p>　　论文中给了一个demo数据的例子，如下图：</p>
<p><img src="/img/plm.jpg" alt=""></p>
<p>　　我们可以通过代码生成类似的样本，来验证一下算法效果：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#demo_data1.py</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">n = int(sys.argv[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    x = <span class="number">2</span> * random() - <span class="number">1.0</span></span><br><span class="line">    y = <span class="number">2</span> * random() - <span class="number">1.0</span></span><br><span class="line">    label = <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> abs(x) + abs(y) &lt; <span class="number">1.0</span>:</span><br><span class="line">        label = <span class="number">0</span></span><br><span class="line">    <span class="keyword">print</span> str(label) + <span class="string">" x:"</span> + str(x) + <span class="string">" y:"</span> + str(y)</span><br></pre></td></tr></table></figure></p>
<p>　　生成1万条训练样本和2000条测试样本：</p>
<p>　　<code>python demo_data1.py 10000 &gt; train.txt</code></p>
<p>　　<code>python demo_data1.py 2000 &gt; test.txt</code></p>
<p>　　alphaPLM的训练参数如下：</p>
<p>　　<code>cat train.txt | ./plm_train -m model.txt -u_bias 1 -w_bias 1 -u_l1 0.001 -u_l2 0.1 -w_l1 0.001 -w_l2 0.1 -core 1 -piece_num 4 -u_stdev 1 -w_stdev 1 -u_alpha 10 -w_alpha 10</code></p>
<p>　　在测试集的AUC可以达到0.99以上。</p>
<p>　　如果是LR或FM，你会发现无论你怎么调参，AUC始终在0.5左右。</p>
<p>　　直观上也很容易理解，看图就会发现，如果特征就是x和y的坐标值的话，数据非线性可分，而很明显在四个象限的分片里分别都是线性可分的。</p>
<p>　　你心里是否已经忍不住开始唾弃LR：“啊呸，LR果然是个战五渣！连这么个demo数据都搞不定！”那你可就冤枉LR了，原始特征非线性，完全可以通过转换变成线性或近似线性，比如做个简单的离散化就会大不一样：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#demo_data2.py</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">n = int(sys.argv[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    x = <span class="number">2</span> * random() - <span class="number">1.0</span></span><br><span class="line">    y = <span class="number">2</span> * random() - <span class="number">1.0</span></span><br><span class="line">    label = <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> abs(x) + abs(y) &lt; <span class="number">1.0</span>:</span><br><span class="line">        label = <span class="number">0</span></span><br><span class="line">    fx = <span class="string">"x"</span> + str(int(x*<span class="number">10</span>)) + <span class="string">":1"</span></span><br><span class="line">    fy = <span class="string">"y"</span> + str(int(y*<span class="number">10</span>)) + <span class="string">":1"</span></span><br><span class="line">    <span class="keyword">print</span> str(label) + <span class="string">" "</span> + fx + <span class="string">" "</span> + fy</span><br></pre></td></tr></table></figure></p>
<p>　　重新生成1万条训练样本和2000条测试样本：</p>
<p>　　<code>python demo_data2.py 10000 &gt; train.txt</code></p>
<p>　　<code>python demo_data2.py 2000 &gt; test.txt</code></p>
<p>　　此时你会发现，无论LR、FM还是PLM，AUC都很容易达到0.99以上。</p>
<p>　　而在我们广告业务的真实数据中，LR所面对的几乎都是千万维或上亿维的高维离散特征，效果并不会比FM或PLM差太多。我在真实数据的实验也验证了这一点，PLM和FM相比LR提升的都差不多，AUC基本都是在千分位提高几个点。</p>

          
        
      </div>
      <footer>
        
          <div class="alignright">
            <a href="/2017/06/01/mlr_plm/#more" class="more-link">Continue Reading<i class="fa fa-long-arrow-right fa-1"></i></a>
          </div>
        
        <div class="clearfix"></div>
      </footer>
    </div>
</article>



  

  <nav id="pagination">
  
  
  <div class="clearfix"></div>
</nav>




   </div></div>
    <aside id="sidebar" class="alignright"><div class="padding">
	
	  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="搜尋">
    <input type="hidden" name="q" value="site:yoursite.com">
  </form>
</div>
	
	  
<div class="widget recent-post">
  <h3 class="title">最新文章</h3>
  <ul class="entry">
    
      <li>
        <a href="/2017/07/16/lambdafm/">lambdaFM</a>
      </li>
    
      <li>
        <a href="/2017/06/01/mlr_plm/">MLR, PLM</a>
      </li>
    
      <li>
        <a href="/2016/11/22/ensembling_lagrange/">Ensembling, Lagrange</a>
      </li>
    
      <li>
        <a href="/2016/10/16/fm_ftrl_softmax/">FM, FTRL, Softmax</a>
      </li>
    
      <li>
        <a href="/2016/02/05/cf_als/">CF的ALS算法推导</a>
      </li>
    
  </ul>
</div>

	
	  
	
	  
	
</div></aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="padding">
	<div class="alignleft">
	  
	  &copy; 2017 CastellanZhang
	  
	  Powerd by <a href="http://hexo.io/" target="_blank">hexo</a>
	  and Theme by <a href="https://github.com/halfer53/metro-light" target="_blank">metro-light</a>
	</div>

	<div class="alignright">
		
		
		
		
		
		
		
	</div>

	<div class="clearfix"></div>
</div>

<div class="scroll-top"><i class="fa fa-arrow-circle-up"></i></div></footer>
  


<script src="//cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/3.0.4/jquery.imagesloaded.js"></script>
<script src="/js/gallery.js"></script>



<script type="text/javascript">
$(window).scroll(function() {

    if($(this).scrollTop() > 400) {
        $('.scroll-top').fadeIn(200);
    } else {
        $('.scroll-top').fadeOut(200);
    }
});

$('.scroll-top').bind('click', function(e) {
    e.preventDefault();
    $('body,html').animate({scrollTop:0},200);
});
</script>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
